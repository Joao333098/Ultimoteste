const axios = require('axios');
require('dotenv').config();

// Configura√ß√£o da API GLM-4
const GLM4_CONFIG = {
    API_KEY: process.env.GLM4_API_KEY || 'seu_api_key_aqui',
    ENDPOINT: 'https://open.bigmodel.cn/api/paas/v4/chat/completions',
    MODEL: 'glm-4'
};

// Cliente para a API GLM-4
class GLM4Client {
    constructor(apiKey) {
        this.apiKey = apiKey;
    }

    /**
     * Envia mensagem para o GLM-4
     * @param {string} message - Mensagem do usu√°rio
     * @param {Array} history - Hist√≥rico da conversa
     * @param {Object} options - Op√ß√µes adicionais
     * @returns {Promise<string>} - Resposta da IA
     */
    async sendMessage(message, history = [], options = {}) {
        try {
            const messages = [
                {
                    role: "system",
                    content: "Voc√™ √© um assistente AI √∫til e prestativo. Responda de forma clara e concisa."
                },
                ...history,
                {
                    role: "user",
                    content: message
                }
            ];

            const response = await axios.post(
                GLM4_CONFIG.ENDPOINT,
                {
                    model: GLM4_CONFIG.MODEL,
                    messages: messages,
                    temperature: options.temperature || 0.7,
                    max_tokens: options.max_tokens || 1000,
                    web_search: options.web_search || false
                },
                {
                    headers: {
                        'Authorization': `Bearer ${this.apiKey}`,
                        'Content-Type': 'application/json'
                    },
                    timeout: 30000
                }
            );

            return response.data.choices[0].message.content;
        } catch (error) {
            console.error('Erro na API GLM-4:', {
                message: error.message,
                status: error.response?.status,
                data: error.response?.data
            });
            throw error;
        }
    }

    /**
     * Exemplo de chat cont√≠nuo
     * @param {Array} initialHistory - Hist√≥rico inicial
     */
    async startChat(initialHistory = []) {
        let history = initialHistory;
        
        console.log('üí¨ Chat com GLM-4 iniciado! Digite "sair" para terminar.\n');
        
        while (true) {
            const readline = require('readline').createInterface({
                input: process.stdin,
                output: process.stdout
            });

            const userMessage = await new Promise(resolve => {
                readline.question('üë§ Voc√™: ', resolve);
            });

            readline.close();

            if (userMessage.toLowerCase() === 'sair') {
                console.log('üëã Chat encerrado!');
                break;
            }

            try {
                console.log('üîÑ Processando...');
                
                const response = await this.sendMessage(userMessage, history, {
                    temperature: 0.7,
                    web_search: true
                });

                console.log(`ü§ñ GLM-4: ${response}\n`);

                // Atualizar hist√≥rico
                history.push(
                    { role: "user", content: userMessage },
                    { role: "assistant", content: response }
                );

                // Manter apenas as √∫ltimas 10 mensagens no hist√≥rico
                if (history.length > 20) {
                    history = history.slice(-20);
                }

            } catch (error) {
                console.log('‚ùå Erro:', error.message);
            }
        }
    }
}

// Exemplos de uso
async function demonstrateGLM4() {
    const client = new GLM4Client(GLM4_CONFIG.API_KEY);

    console.log('üöÄ Demonstra√ß√£o da API GLM-4\n');

    // Exemplo 1: Pergunta simples
    try {
        console.log('1. üìù Pergunta simples:');
        const response1 = await client.sendMessage("Explique o que √© intelig√™ncia artificial em 2 frases.");
        console.log(`Resposta: ${response1}\n`);
    } catch (error) {
        console.log('Erro no exemplo 1:', error.message);
    }

    // Exemplo 2: Com web search
    try {
        console.log('2. üåê Com pesquisa na web:');
        const response2 = await client.sendMessage(
            "Quais as not√≠cias mais recentes sobre tecnologia?",
            [],
            { web_search: true }
        );
        console.log(`Resposta: ${response2}\n`);
    } catch (error) {
        console.log('Erro no exemplo 2:', error.message);
    }

    // Exemplo 3: Com hist√≥rico de conversa
    try {
        console.log('3. üí≠ Conversa com contexto:');
        const history = [
            {
                role: "user",
                content: "Meu nome √© Jo√£o e gosto de programa√ß√£o."
            },
            {
                role: "assistant",
                content: "Ol√° Jo√£o! √â √≥timo saber que voc√™ gosta de programa√ß√£o. Em que linguagens voc√™ trabalha?"
            }
        ];

        const response3 = await client.sendMessage(
            "Estudo JavaScript e Python. Qual voc√™ recomenda para iniciantes?",
            history
        );
        console.log(`Resposta: ${response3}\n`);
    } catch (error) {
        console.log('Erro no exemplo 3:', error.message);
    }

    // Exemplo 4: Chat interativo
    console.log('4. üí¨ Chat interativo:');
    await client.startChat();
}

// Tipos de mensagens suportadas pela API
const MESSAGE_TYPES = {
    SYSTEM: {
        role: "system",
        description: "Define o comportamento do assistente"
    },
    USER: {
        role: "user", 
        description: "Mensagem do usu√°rio"
    },
    ASSISTANT: {
        role: "assistant",
        description: "Resposta da IA"
    }
};

// Exemplo de formato de mensagem
const exampleMessage = {
    role: "user",
    content: "Ol√°, como voc√™ funciona?"
};

// Executar demonstra√ß√£o
if (require.main === module) {
    demonstrateGLM4().catch(console.error);
}

module.exports = { GLM4Client, MESSAGE_TYPES };